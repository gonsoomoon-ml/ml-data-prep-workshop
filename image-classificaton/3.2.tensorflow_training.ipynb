{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 3.2] SageMaker TensorFlow 훈련 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download | Structure | Preprocessing (TensorFlow) | **Train Model (TensorFlow)** (4단계 중의 4/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [알림] <font coler=\"red\"> conda_tensorflow2_p36 커널 </font> 과 함께 사용해야 합니다.\n",
    "\n",
    "* 이 노트북은 `1.1.download_data`, `1.2.structuring_data` 및 `3.1.tensorflow_preprocessing`으로 시작하는 일련의 노트북의 일부입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 노트북 요약\n",
    "---\n",
    "- SageMaker 에서 관리형 EC2 인스턴스로 훈련을 하기 위해서 \"훈련 스크립트\"의 주요한 내용을 확인 합니다.\n",
    "- SageMaker Estimator 설정을 하고 모델 훈련을 합니다.\n",
    "- 훈련된 모델의 가중치를 S3에서 다운로드 받아서 모델을 생성합니다.\n",
    "- 테스트 데이터 셋트를 생성 합니다.\n",
    "- 생성된 모델 및 테스트 세트를 통해서 추론을 하고 실제 예측이 잘 되었는지를 확인 합나다.\n",
    "\n",
    "### 참고\n",
    "- SageMaker-Tensorflow-Step-by-Step 워크샵 \n",
    "    - 세이지 메이커 TF Getting Started, Horovod, Data Distributed Parallelism 포함\n",
    "    - https://github.com/gonsoomoon-ml/SageMaker-Tensorflow-Step-By-Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 0. 환경 설정\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow :  2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "print(f\"tensorflow :  {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버킷 이름 및 훈련/검증 데이터 셋 로딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket_name\n",
    "%store -r train_tf_s3_uri\n",
    "%store -r val_tf_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 훈련 스크립트 리뷰\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper 함수\n",
    "\n",
    "이러한 도우미 함수는 훈련 전에 TFRecords 데이터 세트에 수행해야 하는 변환을 정의합니다. 더 자세한 정보는 이 시리즈의 전처리 가이드를 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtfrecord_parser\u001b[39;49;00m(record):\n",
      "    features = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mheight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mwidth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdepth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mimage_raw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.string),\n",
      "    }\n",
      "    parsed_features = tf.io.parse_single_example(record, features)\n",
      "    \u001b[34mreturn\u001b[39;49;00m tf.io.decode_jpeg(parsed_features[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage_raw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]), parsed_features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32maugment\u001b[39;49;00m(image, label):\n",
      "    image = tf.image.random_flip_left_right(image)\n",
      "    image = tf.image.random_flip_up_down(image)\n",
      "    image = tf.image.random_brightness(image, \u001b[34m0.2\u001b[39;49;00m)\n",
      "    image = tf.image.random_hue(image, \u001b[34m0.1\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m (image, label)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 7,27p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 스크립트 메인 함수\n",
    "훈련 스크립트는 if 문에서 훈련 코드를 래핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 29p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력 파라미터 분석\n",
    "이러한 입력 파라미터는 하이퍼파라미터 인수와 fit 메소드에 대한 입력 인수를 통해 전달됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    arg_parser = argparse.ArgumentParser()\n",
      "    arg_parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m)\n",
      "    arg_parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m)\n",
      "    arg_parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m)\n",
      "\n",
      "    arg_parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    arg_parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--validation-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 29,39p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 병렬화 구성을 위해 autotune 사용\n",
    "- 훈련 속도를 높이기 위해 TensorFlow는 특정 작업을 여러 코어에 분산할 수 있습니다. 작업을 분산할 최적의 작업자 수를 결정하는 것은 어려울 수 있습니다(너무 적으면 GPU 활용도가 낮고 너무 많으면 작업 예약 오버헤드로 인해 지연이 발생함). \n",
    "- TensorFlow는 훈련을 수행하는 컴퓨터를 기반으로 적절한 양을 결정하는 방법과 함께 제공됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 42p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터세트 로드\n",
    "- 훈련 및 검증 데이터세트가 로드됩니다. \n",
    "- 이미지를 TFRecord 파일로 변환할 때 크기 조정이나 스케일을 했기 때문에 다시 수행할 필요가 없습니다.\n",
    "- 훈련 데이터에는 증강이 적용되지만 검증 데이터에는 적용되지 않습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_ds = tf.data.TFRecordDataset(\n",
      "        filenames=[train_data.as_posix()], num_parallel_reads=AUTOTUNE\n",
      "    )\n",
      "\n",
      "    val_ds = tf.data.TFRecordDataset(filenames=[val_data.as_posix()], num_parallel_reads=AUTOTUNE)\n",
      "\n",
      "    train_ds = (\n",
      "        train_ds.map(tfrecord_parser, num_parallel_calls=AUTOTUNE)\n",
      "        .map(augment, num_parallel_calls=AUTOTUNE)\n",
      "        .batch(args.batch_size)\n",
      "        .prefetch(AUTOTUNE)\n",
      "    )\n",
      "\n",
      "    val_ds = (\n",
      "        val_ds.map(tfrecord_parser, num_parallel_calls=AUTOTUNE)\n",
      "        .batch(args.batch_size)\n",
      "        .prefetch(AUTOTUNE)\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 47,65p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU를 사용할 수 있는지 확인\n",
    "GPU를 사용할 수 있는 경우 훈련 장치를 GPU로 설정하고, 그렇지 않으면 CPU를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gpu_devices = tf.config.experimental.list_physical_devices(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36many\u001b[39;49;00m(gpu_devices):\n",
      "        device = gpu_devices[\u001b[34m0\u001b[39;49;00m].device_type\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33m/cpu:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining with: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdevice\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 66,71p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 모델 생성 및 수정\n",
    "- 먼저 장치 컨텍스트를 설정하여 적절한 장치(GPU 또는 CPU)를 사용하고 있는지 확인합니다. \n",
    "- 그런 다음 ResNet50 아키텍처를 사용하고 ImageNet 데이터 세트에서 사전 훈련된 가중치로 가중치를 초기화합니다. \n",
    "- Pretained 모델의 최상위 계층은 ImageNet 이미지에 대해 구성되어 있으므로 분류 계층(`inlcude_top=False`)을 제거\n",
    "- 11마리 동물에 대한 분류 계층으로 교체해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.device(device):\n",
      "\n",
      "        base_model = tf.keras.applications.ResNet50(include_top=\u001b[34mFalse\u001b[39;49;00m, weights=\u001b[33m\"\u001b[39;49;00m\u001b[33mimagenet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        global_avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
      "        output = tf.keras.layers.Dense(\u001b[34m11\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(global_avg)\n",
      "        model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 72,80p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 옵티마이저 정의 및 모델 학습\n",
    "이 예에서는 SGD를 사용하여 모델의 가중치를 최적화합니다. 훈련이 끝나면 최고의 검증 정확도를 수행한 에포크에 대한 가중치가 저장되므로 나중에 테스트 데이터 세트에 대한 예측을 위해 모델을 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        optimizer = tf.keras.optimizers.SGD(lr=args.learning_rate, momentum=\u001b[34m0.9\u001b[39;49;00m, decay=\u001b[34m0.01\u001b[39;49;00m)\n",
      "\n",
      "        model.compile(\n",
      "            loss=\u001b[33m\"\u001b[39;49;00m\u001b[33msparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, optimizer=optimizer, metrics=[\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        )\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBeginning Training...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        model.fit(train_ds, epochs=args.epochs, validation_data=val_ds, verbose=\u001b[34m2\u001b[39;49;00m)\n",
      "\n",
      "        model.save(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 80,90p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SageMaker Estimator 설정 \n",
    "___\n",
    "\n",
    "교육에 사용할 리소스와 리소스 구성 방법을 정의합니다. 상세 사항은 아래를 참조 하세요.\n",
    "\n",
    "- [TensorFlow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='idg4b.2'></a>\n",
    "### 알고리즘 하이퍼파라미터\n",
    "___\n",
    "- 하이퍼파라미터는 훈련이 시작되기 전에 설정한 알고리즘의 튜닝 파라미터를 나타냅니다. 일반적으로 기본값으로 미리 설정되어 있으므로 지정하지 않으면 훈련 알고리즘을 계속 실행할 수 있지만 일반적으로 최적의 결과를 얻으려면 조정이 필요합니다. 이러한 값이 무엇인지는 전적으로 데이터 세트에 따라 다릅니다. \n",
    "- 불행히도 최고의 설정이 무엇인지 알려주는 공식은 없습니다. 직접 시도하고 결과를 확인해야 하지만 선택하는 데 도움이 되는 모범 사례와 팁이 있습니다.\n",
    "\n",
    "* **학습률** - 교육의 각 배치 후에 우리는 해당 배치에 대해 가능한 최상의 결과를 제공하기 위해 모델의 가중치를 업데이트합니다. 학습률은 가중치를 업데이트해야 하는 정도를 제어합니다. 모범 사례는 0.2에서 .001 사이의 값을 지정하며 일반적으로 1보다 높지 않습니다. 학습률이 높을수록 훈련이 최적의 가중치로 더 빨리 수렴되지만 너무 빠르면 목표를 초과할 수 있습니다. 이 예에서는 사전 훈련된 모델의 가중치를 사용하므로 가중치가 이미 최적화되어 있고 가중치에서 너무 멀리 이동하고 싶지 않기 때문에 더 낮은 학습률로 시작하려고 합니다.\n",
    "\n",
    "* **에포크** - 에포크는 훈련 세트의 한 주기를 나타내며 훈련할 에포크가 많다는 것은 정확도를 향상시킬 기회가 더 많다는 것을 의미합니다. 적절한 값은 시간과 예산 제약에 따라 5~25 Epoch 범위입니다. 이상적으로는 검증 정확도가 안정되기 직전에 값이 올바른 Epoch 수 입니다.\n",
    "\n",
    "* **Batch Size** - 일괄 학습은 RAM에 보관해야 하는 데이터의 양을 줄이고 학습 알고리즘의 속도를 높일 수 있습니다. 이러한 이유로 훈련 데이터는 거의 항상 일괄 처리됩니다. 최적의 배치 크기는 데이터 세트, 이미지 크기 및 훈련 컴퓨터의 RAM 용량에 따라 다릅니다. 우리와 같은 데이터 세트의 경우 합리적인 값은 배치당 18개에서 64개 이미지입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련을 위한 하이퍼파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch-size\": 32,\n",
    "    \"learning-rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator 설정\n",
    "\n",
    "현재 노트북의 TF Version 으로 훈련 EC2 인스턴스안으로 다운로드 받을 TF Docker image의 버전을 맞춥니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_config = {\n",
    "    \"entry_point\": \"tensorflow_train.py\",\n",
    "    \"source_dir\": \"training_tensorflow\",\n",
    "    \"framework_version\": \"2.3\",\n",
    "    \"py_version\": \"py37\",\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"instance_count\": 1,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"output_path\": f\"s3://{bucket_name}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(**estimator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 및 검증 데이터 채널 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_channels = {\n",
    "    \"training\": train_tf_s3_uri,\n",
    "    \"validation\": val_tf_s3_uri,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 훈련\n",
    "- 총 소요시간이 약 5분 걸립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-04 01:13:45 Starting - Starting the training job...\n",
      "2022-01-04 01:13:56 Starting - Launching requested ML instances......\n",
      "2022-01-04 01:14:49 Starting - Preparing the instances for training......\n",
      "2022-01-04 01:16:12 Downloading - Downloading input data...\n",
      "2022-01-04 01:16:30 Training - Downloading the training image.........\n",
      "2022-01-04 01:18:12 Training - Training image download completed. Training in progress..\u001b[34m2022-01-04 01:18:13.157136: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:13.161544: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:13.409340: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:13.506662: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:16,838 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:22,017 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-datasets==3.2.1\n",
      "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting promise\u001b[0m\n",
      "\u001b[34m  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (1.18.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wrapt in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (1.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (3.17.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (2.24.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.5.0-py3-none-any.whl (48 kB)\u001b[0m\n",
      "\u001b[34mCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.7/site-packages (from tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==3.2.1->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.54.0-py2.py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=daaba3d20b395e27fb4e7ed3dfcf2fe948c7ecdd7ac2c04c48315d1716a71937\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=ed047bc0f701c17047c9739e5062c3743d5b8f0e28c8cf85a66c002b9e627281\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\u001b[0m\n",
      "\u001b[34mSuccessfully built future promise\u001b[0m\n",
      "\u001b[34mInstalling collected packages: googleapis-common-protos, tqdm, tensorflow-metadata, promise, future, tensorflow-datasets\u001b[0m\n",
      "\u001b[34mSuccessfully installed future-0.18.2 googleapis-common-protos-1.54.0 promise-2.3 tensorflow-datasets-3.2.1 tensorflow-metadata-1.5.0 tqdm-4.62.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:28,936 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"learning-rate\": 0.001,\n",
      "        \"model_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/model\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2022-01-04-01-13-25-978\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"tensorflow_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"tensorflow_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":32,\"epochs\":10,\"learning-rate\":0.001,\"model_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=tensorflow_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=tensorflow_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":32,\"epochs\":10,\"learning-rate\":0.001,\"model_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2022-01-04-01-13-25-978\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/source/sourcedir.tar.gz\",\"module_name\":\"tensorflow_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"tensorflow_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"32\",\"--epochs\",\"10\",\"--learning-rate\",\"0.001\",\"--model_dir\",\"s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 tensorflow_train.py --batch-size 32 --epochs 10 --learning-rate 0.001 --model_dir s3://sagemaker-ap-northeast-2-057716757052/tensorflow-training-2022-01-04-01-13-25-978/model\u001b[0m\n",
      "\u001b[34mTraining with: GPU\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:36.840 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:37.148 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[0m\n",
      "\u001b[34m#015    8192/94765736 [..............................] - ETA: 19s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015   49152/94765736 [..............................] - ETA: 1:46#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  155648/94765736 [..............................] - ETA: 1:04#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  458752/94765736 [..............................] - ETA: 32s #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 1343488/94765736 [..............................] - ETA: 14s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 3538944/94765736 [>.............................] - ETA: 6s #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 6078464/94765736 [>.............................] - ETA: 4s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 8421376/94765736 [=>............................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01510829824/94765736 [==>...........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01513295616/94765736 [===>..........................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01515712256/94765736 [===>..........................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01518186240/94765736 [====>.........................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01520660224/94765736 [=====>........................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01523085056/94765736 [======>.......................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01525559040/94765736 [=======>......................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01528033024/94765736 [=======>......................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01530556160/94765736 [========>.....................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01533046528/94765736 [=========>....................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01535577856/94765736 [==========>...................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01538060032/94765736 [===========>..................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01540599552/94765736 [===========>..................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01543122688/94765736 [============>.................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01545678592/94765736 [=============>................] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01548218112/94765736 [==============>...............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01550823168/94765736 [===============>..............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01553362688/94765736 [===============>..............] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01555902208/94765736 [================>.............] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01558490880/94765736 [=================>............] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01560981248/94765736 [==================>...........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01563602688/94765736 [===================>..........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01566125824/94765736 [===================>..........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01568681728/94765736 [====================>.........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01571155712/94765736 [=====================>........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01573760768/94765736 [======================>.......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01576365824/94765736 [=======================>......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01578872576/94765736 [=======================>......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01581510400/94765736 [========================>.....] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01584123648/94765736 [=========================>....] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01586843392/94765736 [==========================>...] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01589391104/94765736 [===========================>..] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01592086272/94765736 [============================>.] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01594699520/94765736 [============================>.] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01594773248/94765736 [==============================] - 2s 0us/step\u001b[0m\n",
      "\u001b[34mBeginning Training...\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:42.182 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:42.183 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:42.184 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:42.184 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:42.184 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO hook.py:425] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[2022-01-04 01:18:42.223 ip-10-0-91-75.ap-northeast-2.compute.internal:62 INFO hook.py:425] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34mEpoch 1/10\u001b[0m\n",
      "\u001b[34m69/69 - 10s - loss: 1.5256 - accuracy: 0.4941 - val_loss: 0.8410 - val_accuracy: 0.7127 - batch: 0.0000e+00\u001b[0m\n",
      "\u001b[34mEpoch 2/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.8068 - accuracy: 0.7405 - val_loss: 0.6910 - val_accuracy: 0.7782 - batch: 1.0000\u001b[0m\n",
      "\u001b[34mEpoch 3/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.6502 - accuracy: 0.7973 - val_loss: 0.6158 - val_accuracy: 0.8255 - batch: 2.0000\u001b[0m\n",
      "\u001b[34mEpoch 4/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.5510 - accuracy: 0.8291 - val_loss: 0.5764 - val_accuracy: 0.8327 - batch: 3.0000\u001b[0m\n",
      "\u001b[34mEpoch 5/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.4851 - accuracy: 0.8573 - val_loss: 0.5447 - val_accuracy: 0.8400 - batch: 4.0000\u001b[0m\n",
      "\u001b[34mEpoch 6/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.4552 - accuracy: 0.8655 - val_loss: 0.5264 - val_accuracy: 0.8509 - batch: 5.0000\u001b[0m\n",
      "\u001b[34mEpoch 7/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.4028 - accuracy: 0.8859 - val_loss: 0.5182 - val_accuracy: 0.8545 - batch: 6.0000\u001b[0m\n",
      "\u001b[34mEpoch 8/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.3959 - accuracy: 0.8859 - val_loss: 0.5238 - val_accuracy: 0.8545 - batch: 7.0000\u001b[0m\n",
      "\u001b[34mEpoch 9/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.3477 - accuracy: 0.9118 - val_loss: 0.5072 - val_accuracy: 0.8545 - batch: 8.0000\u001b[0m\n",
      "\u001b[34mEpoch 10/10\u001b[0m\n",
      "\u001b[34m69/69 - 9s - loss: 0.3275 - accuracy: 0.9191 - val_loss: 0.5049 - val_accuracy: 0.8582 - batch: 9.0000\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:29.285194: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:29.285327: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:18:29.328173: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:20:34.455558: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/model/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/model/assets\u001b[0m\n",
      "\u001b[34m2022-01-04 01:20:50,843 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving containers. The SavedModel bundle is under directory \"model\", not a numeric name.\u001b[0m\n",
      "\u001b[34m2022-01-04 01:20:50,843 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-01-04 01:20:55 Uploading - Uploading generated training model\n",
      "2022-01-04 01:21:28 Completed - Training job completed\n",
      "Training seconds: 316\n",
      "Billable seconds: 316\n"
     ]
    }
   ],
   "source": [
    "tf_estimator.fit(s3_data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 훈련된 모델 로드 및 테스트 데이터 로딩\n",
    "___\n",
    "\n",
    "- 모델 가중치는 S3에 저장이 되고, 다운로드 후에 모델에 가중치를 다시 로드하여 예측을 생성할 수 있습니다. \n",
    "- 훈련 후에 테스트 데이터에서 모델을 평가하는 것이 중요합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3에서 훈련된 모델 다운로드 및 압축 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader().download(tf_estimator.model_data, \"training_tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tfile = tarfile.open(\"training_tensorflow/model.tar.gz\")\n",
    "tfile.extractall(\"training_tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련된 모델 로딩\n",
    "- 아래 셀을 실행을 하고, 셀이 더 이상 진행이 안되는 경우가 발생할 수 있습니다. \n",
    "- 이러한 이유 중의 하나는 OOM (Out of Memory) 이 발생한 상황일 수 있습니다. \n",
    "- 메모리 확보(다른 노트북 커널 셧다운 혹은 프로세스 제거 등)를 하여 다시 실행하시기 바랍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = tf.keras.models.load_model(\"training_tensorflow/model\")\n",
    "except Exception:\n",
    "    import traceback\n",
    "    traceback.print_exc()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측을 위해 테스트 데이터 세트에서 이미지 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = tfds.ImageFolder(\"./data_structured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_parser(record):\n",
    "    features = {\n",
    "        \"height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"depth\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"image_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(record, features)\n",
    "    return tf.io.decode_jpeg(parsed_features[\"image_raw\"]), parsed_features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.TFRecordDataset(filenames=[\"data_tfrecord/test.tfrecord\"], num_parallel_reads=2)\n",
    "test_ds = test_ds.map(tfrecord_parser, num_parallel_calls=2).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 예측(0에서 9까지)을 원래 클래스 이름(곰에서 얼룩말까지)에 다시 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bear',\n",
       " 1: 'bird',\n",
       " 2: 'cat',\n",
       " 3: 'cow',\n",
       " 4: 'dog',\n",
       " 5: 'elephant',\n",
       " 6: 'frog',\n",
       " 7: 'giraffe',\n",
       " 8: 'horse',\n",
       " 9: 'sheep',\n",
       " 10: 'zebra'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"pickled_data/category_labels.pickle\", \"rb\") as f:\n",
    "    category_labels = pickle.load(f)\n",
    "\n",
    "category_labels = {idx: name for idx, name in enumerate(sorted(category_labels.values()))}\n",
    "category_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.모델 추론과 함께 검증 이미지 표시\n",
    "더 많은 예측을 보려면 셀을 다시 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv1_conv/Conv2D (defined at <ipython-input-28-9be48a761b15>:6) ]] [Op:__inference_predict_function_32540]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9be48a761b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpred_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategory_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m               *args, **kwds)\n\u001b[1;32m    845\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv1_conv/Conv2D (defined at <ipython-input-28-9be48a761b15>:6) ]] [Op:__inference_predict_function_32540]\n\nFunction call stack:\npredict_function\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(10, 7))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    sample = next(iter(test_ds))\n",
    "    image = sample[0]\n",
    "    pred = model.predict(tf.expand_dims(image, axis=0))\n",
    "    pred_name = category_labels[np.argmax(pred)]\n",
    "    ax.imshow(image)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"prediction: {pred_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 단계\n",
    "\n",
    "- 이것으로 SageMaker의 TensorFlow 프레임워크에 대한 이미지 데이터 가이드를 마칩니다. \n",
    "- 모델을 배포하고 테스트 데이터에 대한 예측을 얻으려면 진행에 필요한 모든 정보를 여기에서 찾을 수 있습니다. \n",
    "\n",
    "- [추론을 위한 모델배포](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
